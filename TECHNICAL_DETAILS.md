# 技術詳細解説

このドキュメントは、`build_index.py` と `score_related.py` の背後にある技術的な原理、各パラメータの詳細、そして応用的なチューニング方法について詳しく解説します。

---

## 1. 技術的原理

このシステムは、日本語の文書群から関連文書を高速かつ高精度に発見するために、以下の技術を組み合わせています。

### 1.1. 文字 N-gram

**N-gram** とは、テキストを連続するN個の文字（または単語）で分割する手法です。例えば、「今日の天気」というテキストに文字 3-gram（`ngram=3`）を適用すると、以下のようになります。

- `「今日」`
- `「日の天」`
- `「の天気」`

**なぜ文字 N-gram なのか？**

日本語のテキスト処理で最も一般的なのは、MeCab や Janome といった形態素解析器で単語に分割（分かち書き）する手法です。しかし、このアプローチにはいくつかの課題があります。

- **辞書依存**: 新語、専門用語、固有名詞が辞書にないと正しく分割できません。
- **処理速度**: 形態素解析は計算コストが比較的高く、大量の文書を処理する際にボトルネックになり得ます。
- **導入の手間**: 外部ライブラリのインストールや設定が必要です。

一方、文字 N-gram は、**分かち書きが不要**という大きな利点があります。これにより、未知語に強く、セットアップが容易で、非常に高速に動作します。特に、3文字程度の固有名詞や専門用語が多く含まれる技術文書やブログ記事などでは、単語ベースのTF-IDFを上回る精度を発揮することがあります。

### 1.2. TF-IDF (Term Frequency-Inverse Document Frequency)

TF-IDF は、ある単語（このシステムでは N-gram）が、特定の文書内でどれだけ重要かを示すスコアです。これは2つの指標から計算されます。

- **TF (Term Frequency, 単語の出現頻度)**:
  - ある文書内で、特定の N-gram がどれだけ多く出現するか。
  - `(特定の N-gram の出現回数) / (文書全体の N-gram 総数)`
  - ある文書で頻出する N-gram ほど、その文書のトピックをよく表していると考えられます。

- **IDF (Inverse Document Frequency, 逆文書頻度)**:
  - その N-gram が、全文書のうちどれだけ多くの文書に出現するか、の逆数。
  - `log(全文書数 / (その N-gram が出現する文書数 + 1))`
  - 「しかし」「です」のような一般的な表現（多くの文書に出現）は重要度が低くなり、「人工知能」「深層学習」のような専門用語（特定の文書に集中）は重要度が高くなります。

**TF-IDF スコア**は `TF * IDF` で計算され、この値が高いほど、その N-gram は「その文書を特徴づける重要なキーワード」であると判断できます。

このシステムでは、`scikit-learn` の `TfidfVectorizer` を用いて、全文書の全 N-gram について TF-IDF スコアを計算し、**文書×N-gram の巨大な行列（疎行列）**を作成します。

### 1.3. コサイン類似度 (Cosine Similarity)

TF-IDF によって、各文書は「N-gram の重要度」を要素に持つ**ベクトル**として表現されます。2つの文書がどれだけ似ているかを測るために、これらのベクトルの向きがどれだけ近いかを計算するのが**コサイン類似度**です。

- 2つのベクトルがなす角度のコサイン（`cos(θ)`）を計算します。
- 完全に同じ向きなら `1`、直交していれば `0`、正反対なら `-1` となります。TF-IDF ベクトルの要素は非負なので、通常は `0` から `1` の範囲に収まります。
- スコアが `1` に近いほど、2つの文書は内容的に類似していると判断できます。

`score_related.py` は、問い合わせ（クエリ）として与えられた HTML 文書を TF-IDF ベクトルに変換し、インデックス内の全文書ベクトルとのコサイン類似度を総当たりで計算して、スコアの高い順に関連文書として出力します。

### 1.4. テキスト抽出と重み付け

HTML 文書からやみくもにテキストを抽出すると、ナビゲーションメニューやフッターといった本質的でないノイズが多く含まれてしまいます。そこで、このシステムでは以下の工夫をしています。

- **不要要素の除去**: `script`, `style` タグや、`--drop-selectors` で指定された CSS セレクタ（例: `nav`, `.footer`）に一致する要素を解析前に削除します。
- **重要要素の重み付け**: 文書の内容を最もよく表すのは `title` タグや見出し（`h1`, `h2`...）です。これらのテキストを本文よりも重視するため、`--title-weight` や `--heading-weight` で指定された回数だけ、テキストを繰り返し連結してから TF-IDF を計算します。これにより、タイトルや見出しに含まれるキーワードの重要度が実質的に高まります。

---

## 2. パラメータ詳細解説

### 2.1. `build_index.py` のパラメータ

| パラメータ | デフォルト値 | 説明 |
| :--- | :--- | :--- |
| **`--src`** | (必須) | インデックス化したい HTML ファイル群が格納されているルートディレクトリ。サブディレクトリも再帰的に探索されます。 |
| **`--out`** | (必須) | 生成されたインデックスファイル群を保存するディレクトリ。存在しない場合は自動的に作成されます。 |
| **`--ngram`** | `3` | 文字 N-gram の N の値。`2` や `4` も試す価値があります。一般的に、`3` が日本語の短いフレーズを捉えるのにバランスが良いとされます。短い単語や固有名詞が多い場合は `2`、より文脈を重視したい場合は `4` が有効なことがあります。 |
| **`--min-df`** | `2` | N-gram が TF-IDF の語彙に含まれるための最小文書頻度 (document frequency)。`2` の場合、最低でも2つの文書に出現しない N-gram は無視されます。これにより、タイプミスや稀すぎる表現といったノイズを除去できます。文書数が非常に少ない場合は `1` に設定します。 |
| **`--max-df`** | `0.95` | N-gram が TF-IDF の語彙に含まれるための最大文書頻度（割合）。`0.95` の場合、全文書の 95% 以上に出現する N-gram は無視されます。これは「です」「ます」のような、ほとんどの文書に登場するが内容の識別に寄与しない「ストップワード」を自動的に除去する効果があります。 |
| **`--title-weight`** | `3` | `<title>` タグのテキストに適用する重み。`3` の場合、タイトル内のテキストが3回繰り返され、TF-IDF 計算における重要度が3倍になります。 |
| **`--heading-weight`** | `2` | `<h1>`, `<h2>`, `<h3>` タグのテキストに適用する重み。タイトルの次に重要視されます。 |
| **`--drop-selectors`** | (長いリスト) | テキスト抽出前に除去する要素の CSS セレクタ（カンマ区切り）。ナビゲーション、フッター、サイドバー、パンくずリストなど、サイト共通の定型的な部分を指定します。対象サイトの構造に合わせて調整することが精度向上に不可欠です。 |

### 2.2. `score_related.py` のパラメータ

| パラメータ | デフォルト値 | 説明 |
| :--- | :--- | :--- |
| **`--index`** | (必須) | `build_index.py` で作成したインデックスディレクトリ。 |
| **`--query`** | (必須) | 関連文書を検索したい対象の HTML ファイル。インデックスに含まれるファイルでも、外部のファイルでも構いません。 |
| **`--topk`** | `10` | 表示する関連文書の最大数。 |
| **`--tau`** | `0.25` | 類似度スコアのしきい値（0〜1）。コサイン類似度がこの値未満の文書は、たとえ `topk` の範囲内でも表示されません。関連性の低い結果を除外したい場合に値を大きくします（例: `0.4`）。 |
| **`--format`** | `table` | 出力形式。人間が読みやすい `table` か、機械処理しやすい `json` を選択できます。 |
| `--drop-selectors` | (同上) | クエリ HTML ファイルを解析する際に適用する除去セレクタ。インデックス作成時と同じ値を指定するのが一般的です。 |
| `--title-weight` | `3` | クエリ HTML ファイルに適用する重み。インデックス作成時と同じ値を指定するのが一般的です。 |
| `--heading-weight` | `2` | クエリ HTML ファイルに適用する重み。インデックス作成時と同じ値を指定するのが一般的です。 |

---

## 3. インデックスファイルの構造

`--out` で指定したディレクトリには、以下のファイルが生成されます。

- **`tfidf_vectorizer.pkl`**: `TfidfVectorizer` の学習済みインスタンス。語彙（N-gram のリスト）や IDF の値などが含まれており、新しい文書（クエリなど）を同じ基準でベクトル化するために不可欠です。`joblib` でシリアライズされています。
- **`tfidf_matrix.npz`**: 全文書の TF-IDF ベクトルを格納した疎行列。`[文書ID, N-gram_ID] = TF-IDFスコア` という形式のデータを効率的に保持しています。`scipy.sparse` 形式で保存されています。
- **`id_map.json`**: 行列の `文書ID` (0からの連番) と、元のファイル情報（パス、タイトルなど）を対応付ける辞書。
- **`docs.jsonl`**: 各文書のメタデータ（パス、タイトル、文字数、ハッシュ値）を JSONL 形式（1行1JSON）で記録したファイル。
- **`source_root.txt`**: インデックス作成時のソースディレクトリの絶対パス。結果表示時に相対パスから絶対パスを復元するために使われます。

---

## 4. 高度なトピックと今後の拡張

このシステムはシンプルながら強力ですが、さらに精度や機能を向上させるための拡張が考えられます。

- **埋め込みによる再ランク (Re-ranking)**:
  - TF-IDF はキーワードのマッチングには強いですが、「類義語」や「言い換え」を捉えることはできません（例: 「PC」と「パソコン」）。
  - そこで、まず TF-IDF で高速に上位候補（例: 50件）を絞り込み、その候補だけを Sentence-Transformers などの高精度な埋め込みモデルでベクトル化し、再度類似度計算を行う「再ランク」手法が有効です。これにより、計算コストを抑えつつ、意味的な類似性も考慮できます。

- **相互リンクへのボーナス**:
  - 文書AからBへのリンクがあり、かつBからAへのリンクもある場合、これらは強い関連性を持つ可能性が高いです。HTML の `<a>` タグを解析し、このような相互リンク関係にあるペアのスコアにボーナスを加えることで、より文脈に合ったランキングが実現できます。

- **階層的なインデックス**:
  - 長大な文書の場合、文書全体で1つのベクトルを作るのではなく、章や節（`<h2>`, `<h3>` など）ごとにテキストを分割し、それぞれで TF-IDF ベクトルを計算します。
  - これにより、「文書Aの第2章は、文書Bの第5章と関連が深い」といった、よりきめ細やかな関連付けが可能になります。

- **ANN (近似最近傍探索)**:
  - 文書数が数十万を超えると、クエリのたびに全文書とのコサイン類似度を計算する総当たり方式では速度が問題になります。
  - FAISS や HNSWlib のようなライブラリを使い、ANN (Approximate Nearest Neighbor) インデックスを構築することで、わずかな精度低下と引き換えに、検索をミリ秒単位に高速化できます。
